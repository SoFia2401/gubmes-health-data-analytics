# Introduction to Machine Learning

Many of you might have heard of machine learning before, some of you might have even used it! Here we will start with the basics. First we need to know what machine learning is and WHY would we want to use it in Data Science.

### Task 1: 

There are 2 main types of machine learning tasks; supervirsed learning and unsupervirsed learning. Research what the main differences between these and what tasks they might be useful for.

```

#Supervised learning is a machine learning approach that’s defined by its use of labeled datasets. 
These datasets are designed to train or “supervise” algorithms into classifying data or predicting outcomes accurately. 
Using labeled inputs and outputs, the model can measure its accuracy and learn over time.

#Unsupervised learning uses machine learning algorithms to analyze and cluster unlabeled data sets. 
These algorithms discover hidden patterns in data without the need for human intervention (hence, they are “unsupervised”).

```

To start us of with we will be using a supervirsed machine learning method. One of simplest methods is the k-Nearest Neighbors. 


### Task 2

Research the k-Nearest Neighbors algortihtm and how it can be used for classification and regression models, detail your findings below. 

Useful links to start you off with:
https://scikit-learn.org/stable/modules/neighbors.html#classification , 
https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py

```
#Overview
Neighbors-based classification is a type of instance-based learning or non-generalizing learning: 
it does not attempt to construct a general internal model, but simply stores instances of the training data. 
Classification is computed from a simple majority vote of the nearest neighbors of each point: 
a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.

scikit-learn implements two different nearest neighbors classifiers: KNeighborsClassifier implements learning based on the  
k-nearest neighbors of each query point, where k is an integer value specified by the user. RadiusNeighborsClassifier implements
learning based on the number of neighbors within a fixed radius r of each training point, where r is a floating-point value 
specified by the user.

Neighbors-based regression can be used in cases where the data labels are continuous rather than discrete variables. The label
assigned to a query point is computed based on the mean of the labels of its nearest neighbors.

scikit-learn implements two different neighbors regressors: KNeighborsRegressor implements learning based on the k nearest
neighbors of each query point, where k is an integer value specified by the user. RadiusNeighborsRegressor implements
learning based on the neighbors within a fixed radius r of the query point, where r is a floating-point value specified by the user.

```

As you may have noticed k-Neares Neighbors is within the scikit-learn library. This is the most prominent machine learning library in Python and contains state-of the art machine learning methods! You can find a lot of information on their website (https://scikit-learn.org/stable/) so make sure to keep that handy! 
Now we will be implementing our classification algorithm! 

### Task 3: 

Following the example detailed in https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py

Import all functionalities from libraries needed as well as the dataset. First, visualize the iris dataset! What do each of the columns in the data represent?

Then run the code. First plot the boundaries defined by the model without the training data on the plot. Then add the training data to your plot. 

```

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.colors import ListedColormap
from sklearn import neighbors, datasets

n_neighbors = 15

# import some data to play with
iris = datasets.load_iris()

# we only take the first two features. We could avoid this ugly
# slicing by using a two-dim dataset
X = iris.data[:, :2]
y = iris.target

h = 0.02  # step size in the mesh

# Create color maps
cmap_light = ListedColormap(["orange", "cyan", "cornflowerblue"])
cmap_bold = ["darkorange", "c", "darkblue"]

for weights in ["uniform", "distance"]:
    # we create an instance of Neighbours Classifier and fit the data.
    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)
    clf.fit(X, y)

    # Plot the decision boundary. For that, we will assign a color to each
    # point in the mesh [x_min, x_max]x[y_min, y_max].
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    plt.figure(figsize=(8, 6))
    plt.contourf(xx, yy, Z, cmap=cmap_light)

    # Plot also the training points
    sns.scatterplot(
        x=X[:, 0],
        y=X[:, 1],
        hue=iris.target_names[y],
        palette=cmap_bold,
        alpha=1.0,
        edgecolor="black",
    )
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.title(
        "3-Class classification (k = %i, weights = '%s')" % (n_neighbors, weights)
    )
    plt.xlabel(iris.feature_names[0])
    plt.ylabel(iris.feature_names[1])

plt.show() 

```

### This is the end of this training exercise. In order to become proficient with these methodologies you will need PRACTICE! So make sure to look at examples on the scikit-learn website and run the codes yourself!
